{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTFm0fIbv45ggSL9SlGh24",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dishantgupta2004/Deep-Learning-Assignments/blob/main/DishantGupta_CNNFundamentals_Assignments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explain the basic components of a digital image and how it is represented in a computer. State the differences between grayscale and color images.**"
      ],
      "metadata": {
        "id": "wJ5qx7RGoHLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Basic Components of a Digital Image**: A digital image is a two-dimensional representation of a visual scene that is stored and processed on a computer. It is made up of the following basic components:\n",
        "1. **Pixels**: The smallest unit of a digital image. Each pixel represents a single point in the image and holds information about its color or intensity. Images are usually organized in a grid of pixels (rows and columns).\n",
        "2. **Resolution**: The total number of pixels in the image, typically given as width Ã— height (e.g., 1920Ã—1080). Higher resolution means more detail.\n",
        "3. **Bit Depth**: Refers to the number of bits used to represent the color or intensity of each pixel.\n",
        "4. **Common values**: 8-bit grayscale = 256 levels of intensity. 24-bit color = 8 bits per channel for RGB (Red, Green, Blue).\n",
        "5. **Color Model**: Defines how colors are represented. The most common model for digital images is RGB (Red, Green, Blue). Each pixel in a color image is represented by three values corresponding to the R, G, and B channels.\n",
        "\n",
        "**Image Representation in a Computer**: Digital images are stored as arrays (matrices) in memory. For a grayscale image, it is a 2D array where each element is a pixel intensity (e.g., 0 = black, 255 = white). For a color image, it is a 3D array: width Ã— height Ã— 3 (for RGB values). Images can be stored in various file formats such as JPEG, PNG, BMP, etc., which may include compression and metadata.\n",
        "\n",
        "***Difference between Gray Scale and Color Images**\n",
        "\n",
        "| Feature              | Grayscale Image                              | Color Image                                  |\n",
        "|----------------------|----------------------------------------------|----------------------------------------------|\n",
        "| Pixel Value          | Single intensity value (0â€“255)               | Three values (R, G, B), each 0â€“255           |\n",
        "| Memory Requirement   | Less (1 byte per pixel)                      | More (typically 3 bytes per pixel)           |\n",
        "| Bit Depth            | Usually 8 bits                               | Usually 24 bits (8 bits per channel)         |\n",
        "| Visual Content       | Shades of gray (black to white)              | Full range of colors                         |\n",
        "| Array Dimensions     | 2D array (height Ã— width)                    | 3D array (height Ã— width Ã— 3)                |"
      ],
      "metadata": {
        "id": "Qsued2Giofuc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Define Convolutional Neural Networks (CNNs) and discuss their role in image processing.Describe the key advantages of using CNNs over traditional neural networks for image-related tasks.**\n"
      ],
      "metadata": {
        "id": "GOFuIQiTpkLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Definition**: Convolutional Neural Networks (CNNs) are a specialized type of deep learning model designed primarily for processing grid-like data, such as images. They automatically and adaptively learn spatial hierarchies of features from input images through a series of layers. CNNs use a mathematical operation called convolution instead of relying on full connectivity (as in traditional neural networks). This operation helps to extract local features like edges, textures, and shapes.\n",
        "\n",
        "- **Role of CNNs in Image Processing**: CNNs are widely used in image-related tasks such as:\n",
        " 1. Image classification (e.g., identifying objects in photos)\n",
        " 2. Object detection (e.g., locating faces in images)\n",
        " 3. Image segmentation (e.g., separating objects from backgrounds)\n",
        " 4. Facial recognition, medical image analysis, and self-driving car vision systems\n",
        "\n",
        "- **Core components of CNNs**:\n",
        " 1. Convolutional Layers: Apply filters (kernels) to extract local features Learn low-level features (edges) in early layers and high-level features (shapes, objects) in deeper layers.\n",
        " 2. Activation Functions: Typically ReLU (Rectified Linear Unit), which introduces non-linearity.\n",
        " 3. Pooling Layers: Reduce the spatial dimensions (e.g., max pooling). Helps in down-sampling, reducing computation and overfitting.\n",
        " 4. Fully Connected Layers: Flatten the extracted features and perform final classification.\n",
        " 5. Dropout & Batch Normalization: Help improve training efficiency and prevent overfitting. Key Advantages of CNNs Over Traditional Neural Networks\n",
        "\n",
        "| Feature                          | CNNs                                               | Traditional Neural Networks                     |\n",
        "|----------------------------------|----------------------------------------------------|--------------------------------------------------|\n",
        "| **Parameter Efficiency**         | Shared weights reduce the number of parameters     | Full connectivity causes a huge number of weights |\n",
        "| **Translation Invariance**       | Convolution detects features regardless of position| Sensitive to location of features                |\n",
        "| **Feature Hierarchy Learning**   | Learns local to global features in layers          | Requires manual feature extraction               |\n",
        "| **Better Performance**           | Superior accuracy in image-related tasks           | Inferior in complex image understanding tasks    |\n",
        "| **Reduced Overfitting**          | Fewer parameters and pooling reduce overfitting    | More prone to overfitting without regularization |\n",
        "| **Scalability**                  | Scales well to high-resolution images              | Computationally expensive on large images        |\n"
      ],
      "metadata": {
        "id": "d4uVkOH0psqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiWIoh3fn9Ei",
        "outputId": "c9e00e1a-99d3-4d90-90f1-2e7bbd7cd5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 45ms/step - accuracy: 0.3672 - loss: 1.7213 - val_accuracy: 0.5726 - val_loss: 1.1898\n",
            "Epoch 2/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 44ms/step - accuracy: 0.5880 - loss: 1.1603 - val_accuracy: 0.6290 - val_loss: 1.0462\n",
            "Epoch 3/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 45ms/step - accuracy: 0.6509 - loss: 1.0003 - val_accuracy: 0.6384 - val_loss: 1.0126\n",
            "Epoch 4/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 45ms/step - accuracy: 0.6792 - loss: 0.9064 - val_accuracy: 0.6896 - val_loss: 0.9042\n",
            "Epoch 5/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 44ms/step - accuracy: 0.7205 - loss: 0.8030 - val_accuracy: 0.6976 - val_loss: 0.8829\n",
            "Epoch 6/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 45ms/step - accuracy: 0.7414 - loss: 0.7479 - val_accuracy: 0.7012 - val_loss: 0.8908\n",
            "Epoch 7/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 45ms/step - accuracy: 0.7601 - loss: 0.6929 - val_accuracy: 0.7039 - val_loss: 0.8772\n",
            "Epoch 8/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 44ms/step - accuracy: 0.7733 - loss: 0.6484 - val_accuracy: 0.7109 - val_loss: 0.8568\n",
            "Epoch 9/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 45ms/step - accuracy: 0.7908 - loss: 0.6036 - val_accuracy: 0.7160 - val_loss: 0.8672\n",
            "Epoch 10/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 44ms/step - accuracy: 0.8012 - loss: 0.5665 - val_accuracy: 0.7010 - val_loss: 0.9112\n",
            "Epoch 11/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 44ms/step - accuracy: 0.8152 - loss: 0.5258 - val_accuracy: 0.7059 - val_loss: 0.9479\n",
            "Epoch 12/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 46ms/step - accuracy: 0.8267 - loss: 0.4893 - val_accuracy: 0.7143 - val_loss: 0.8906\n",
            "Epoch 13/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 46ms/step - accuracy: 0.8377 - loss: 0.4567 - val_accuracy: 0.7162 - val_loss: 0.8907\n",
            "Epoch 14/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 47ms/step - accuracy: 0.8516 - loss: 0.4189 - val_accuracy: 0.7143 - val_loss: 0.9582\n",
            "Epoch 15/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 46ms/step - accuracy: 0.8631 - loss: 0.3923 - val_accuracy: 0.6975 - val_loss: 1.0301\n",
            "Epoch 16/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 45ms/step - accuracy: 0.8723 - loss: 0.3577 - val_accuracy: 0.7118 - val_loss: 1.0125\n",
            "Epoch 17/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 45ms/step - accuracy: 0.8757 - loss: 0.3432 - val_accuracy: 0.7118 - val_loss: 1.0564\n",
            "Epoch 18/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 45ms/step - accuracy: 0.8876 - loss: 0.3142 - val_accuracy: 0.7057 - val_loss: 1.1025\n",
            "Epoch 19/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 45ms/step - accuracy: 0.8957 - loss: 0.2885 - val_accuracy: 0.7108 - val_loss: 1.1735\n",
            "Epoch 20/20\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 45ms/step - accuracy: 0.9046 - loss: 0.2670 - val_accuracy: 0.7011 - val_loss: 1.2107\n"
          ]
        }
      ],
      "source": [
        "## Implementation:\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10)  # 10 output classes\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=20,\n",
        "                    validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define convolutional layers and their purpose in a CNN.Discuss the concept of filters and how they are applied during the convolution operation.Explain the use of padding and strides in convolutional layers and their impact on the output size."
      ],
      "metadata": {
        "id": "joAURdd4sv14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ“Œ 1. Convolutional Layers and Their Purpose in CNNs\n",
        "A convolutional layer is the fundamental building block of a Convolutional Neural Network (CNN). It performs the convolution operation to extract features from the input data (usually images).\n",
        "\n",
        "ðŸ”¹ Purpose:\n",
        "Automatically learns and detects local features such as edges, textures, and patterns.\n",
        "\n",
        "Preserves the spatial relationship between pixels.\n",
        "\n",
        "Reduces the need for manual feature extraction.\n",
        "\n",
        "Enables deep networks to learn hierarchical representations â€” from low-level (edges) to high-level (shapes, objects).\n",
        "\n",
        "ðŸ“Œ 2. Filters (Kernels) and the Convolution Operation\n",
        "A filter or kernel is a small matrix (e.g., 3Ã—3 or 5Ã—5) of trainable weights used in the convolution operation.\n",
        "\n",
        "ðŸ”¹ How Filters Work:\n",
        "A filter slides over the input image, computing element-wise multiplications with overlapping parts of the input.\n",
        "\n",
        "The results are summed to produce a single value in the output feature map.\n",
        "\n",
        "Each filter is designed to detect a specific feature (e.g., vertical edge, corner, color pattern).\n",
        "\n",
        "Multiple filters are used in each convolutional layer, producing multiple feature maps.\n",
        "\n",
        "ðŸ“Œ 3. Padding in Convolutional Layers\n",
        "Padding refers to adding extra pixels (typically zeros) around the edges of the input image before applying convolution.\n",
        "\n",
        "ðŸ”¹ Types of Padding:\n",
        "Valid Padding: No padding; results in a smaller output.\n",
        "\n",
        "Same Padding: Pads input so output has the same dimensions as input.\n",
        "\n",
        "ðŸ”¹ Why Padding Is Used:\n",
        "To preserve spatial size (important in deep networks).\n",
        "\n",
        "To allow filters to reach edge pixels.\n",
        "\n",
        "To control the size of the output feature map.\n",
        "\n",
        "ðŸ“Œ 4. Strides in Convolutional Layers\n",
        "Stride defines how far the filter moves across the input in each step.\n",
        "\n",
        "ðŸ”¹ Effect of Stride:\n",
        "Stride = 1: Filter slides one pixel at a time â†’ output retains more detail.\n",
        "\n",
        "Stride > 1: Filter jumps by 2+ pixels â†’ output is smaller and more compressed.\n",
        "\n",
        "| Component         | Description                                             | Effect on Output                     |\n",
        "|------------------|---------------------------------------------------------|--------------------------------------|\n",
        "| **Convolutional Layer** | Extracts features using filters                    | Learns patterns in input data        |\n",
        "| **Filter (Kernel)**     | Small matrix of weights for feature detection      | Creates feature maps                 |\n",
        "| **Padding**             | Adds pixels to border                              | Controls output size, retains edges  |\n",
        "| **Stride**              | Step size of filter movement                       | Affects spatial resolution of output |"
      ],
      "metadata": {
        "id": "Km3HeLw-zGgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Describe the purpose of pooling layers in CNNs.Compare max pooling and average pooling operations."
      ],
      "metadata": {
        "id": "MJzO2hMzySGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Purpose of Pooling Layers in CNNs**: Pooling layers are used in Convolutional Neural Networks (CNNs) to downsample feature maps, reducing their spatial dimensions (width and height) while retaining the most important features.\n",
        "- **Main Purposes**:\n",
        " 1. Dimensionality Reduction: Reduces the number of parameters and computation.\n",
        " 2. Translation Invariance: Makes the model more robust to small shifts and distortions.\n",
        " 3. Feature Consolidation: Emphasizes the most dominant features detected by convolutional layers.\n",
        "\n",
        "- **How Pooling Works**: Pooling operates on small patches (usually 2Ã—2) of the input and replaces each patch with a single value based on the type of pooling.\n",
        "\n",
        "| Feature             | Max Pooling                                  | Average Pooling                             |\n",
        "|---------------------|-----------------------------------------------|---------------------------------------------|\n",
        "| **Definition**      | Takes the **maximum** value in each patch     | Takes the **average** of values in each patch |\n",
        "| **Purpose**         | Captures the **strongest activation**         | Captures the **average presence** of features |\n",
        "| **Effect**          | Highlights prominent features (edges, etc.)   | Smooths feature maps, may lose sharp details |\n",
        "| **Output**          | More **sparse** representation                | More **blended** representation              |\n",
        "| **Common Use Case** | Most widely used in modern CNNs               | Occasionally used for smoother features      |"
      ],
      "metadata": {
        "id": "iz-1Qz9_yo5y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i6jo2RKRs1M2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}